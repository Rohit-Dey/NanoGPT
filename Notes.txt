ChatGPT is a language model that predicts sequences of words, characters, or tokens. It understands how words typically relate to one another in the English language.

What is the neural network under the hood that models the sequence of these words? 
GPT stands for Generative Pre-trained Transformer. The Transformer is the neural network architecture that performs the complex computations and processing behind the scenes.

Responses in ChatGPT are generated on a token-by-token basis, where tokens represent smaller subword segments. This means they function at a level that's not purely word-based,
but rather at a chunk level of words. However, in our model, we process it character by character.

We can consider our vocabulary as a set of potential characters that the model can recognize or generate.

When people refer to tokenization, they mean the process of transforming raw text, represented as a string, into a sequence of integers based on a predefined vocabulary of possible elements.

This is just one of many potential encodings or types of tokenizers, and it’s quite basic. There are various other schemas that have been developed in practice. 
For instance, Google employs a tokenizer called SentencePiece, which also converts text into integers but follows a different schema and uses a distinct vocabulary. 
SentencePiece operates at the subword level, meaning it doesn’t encode complete words or individual characters but rather subword units. This subword approach is commonly adopted. 
Additionally, OpenAI has a library called tiktoken that utilizes a byte pair encoding tokenizer, which is the method used by GPT.

Essentially, there is a trade-off between codebook size and sequence length. This means you can opt for very long sequences of integers with a small vocabulary, or you can choose shorter sequences with a larger vocabulary. 
In practice, people commonly use subword encodings to strike a balance between these options.

We want to begin inputting these text sequences or integer sequences into the Transformer for it to learn and recognize patterns. It's important to note that we won’t be feeding the entire text into 
the Transformer all at once, as that would be computationally expensive and impractical. Instead, when training a Transformer with these datasets, we work with smaller portions of the data. 
We randomly sample small chunks from the training set and train on these segments one at a time. Each chunk has a specific length, with a defined maximum length, often referred to as block size in the code, 
though it may also be called context length in other contexts.

We train on all eight examples, utilizing contexts ranging from one to the maximum block size. This approach is not merely for computational efficiency, nor is it just because we already have the sequences available. 
Instead, it is a deliberate strategy to familiarize the Transformer Network with a wide range of contexts. By exposing the model to contexts from as small as one to the full block size—and everything in between—we 
prepare it for more effective inference later on. This means that during the sampling process, we can start with just a single character of context, enabling the Transformer to predict the next character based on 
contexts from one up to the full block size.

We've examined the time dimension of the tensors feeding into the Transformer, but there's one more dimension to consider: the batch dimension. When sampling text chunks, we will feed multiple batches of text stacked 
in a single tensor into the Transformer. This approach is for efficiency, as it keeps the GPUs engaged, leveraging their strength in parallel data processing. Each chunk is processed independently without interaction, 
so let’s generalize this by introducing the batch dimension.

Once we obtain the logits, we concentrate only on the last time step. Instead of maintaining the full shape of B x T x C, we extract the last element along the time dimension, which corresponds to the predictions 
for the next step. This gives us the logits, which we then convert to probabilities using softmax. Finally, we use `torch.multinomial` to sample from these probabilities.

Adam is a more advanced and popular optimizer that works well. A typical good setting for the learning rate is around 3E-4, but for very small networks like this one, much higher learning rates can be effective.

This is the simplest possible model. Currently, the tokens do not interact; they only consider the last character to predict the next one. Now, these tokens need to communicate and incorporate the broader context to improve their predictions. 
This will initiate the Transformer model.

A "bag of words" refers to a method of averaging words, where each of the eight locations holds a word, and we simply average them together. 

We encode not only the identity of tokens but also their positions. Accordingly, there will be a second position embedding table, which is an embedding of block size by an embedding dimension. 
Each position from zero to block size minus one will have its own embedding vector.

Different tokens find other tokens more or less interesting based on data dependency. For instance, a vowel might seek consonants from its past and want information about them to flow in a data-dependent manner. 
Self-attention addresses this by having each token emit two vectors: a query, representing what it seeks, and a key, indicating what it contains. Affinities between tokens are determined through the dot product 
of the keys and queries; when a query aligns closely with a key, the interaction increases, allowing the token to learn more about that specific token compared to others in the sequence.

You can think of X as kind of like private information to this token, so I'm a fifth token at some and I have some identity and my information is kept in Vector X and now for the purposes of the single head here's 
what I'm interested in(Q) here's what I have(K) and if you find me interesting here's what I will communicate to you and that's stored in V and so V is the thing that gets aggregated for the purposes of this
single head.

This is called self-attention because the keys, queries, and values all originate from the same source, X. In this case, the nodes are self-attending. However, attention is generally more versatile. For instance, in encoder-decoder transformers, 
queries can come from X while keys and values may come from a separate external source, such as encoder blocks that provide additional context. Cross attention is utilized when we need to extract information from an external source, while self-attention 
occurs when nodes interact with each other. In this instance, we are using self-attention.

In the context of using softmax in neural networks, it's crucial to ensure that the initial weight values are appropriately set to avoid overly extreme distributions. At initialization, we want the weights to be fairly diffuse; that is, their values should 
be spread out rather than concentrated. This is important because, when softmax is applied to a set of weights that take on very high positive or very low negative values, it can lead to sharp outputs that degenerate into one-hot vectors.
This behavior can be problematic, particularly at the initialization phase. If the values are too extreme and lead to sharp peaks in the softmax output, the network might rely heavily on information from a single node. Ideally, we want each node to aggregate 
information from multiple sources, which allows for a more balanced and effective learning process.
Therefore, controlling the variance at initialization through appropriate scaling of the weights is essential. This scaling helps ensure that the initial distribution does not produce a softmax output that is too peaky, thereby promoting a more effective and 
diverse aggregation of information across the network.

The lower triangular matrix, commonly referred to as 'tril,' is not a parameter of the module in PyTorch. Instead, it is treated as a buffer, which is a type of stateful tensor that does not require gradient updates. To assign this buffer to the module, 
you should use the register_buffer method. This approach ensures that the lower triangular matrix is part of the module's state while not being a learnable parameter.

The self attention can't tolerate very very high learning rates and then I also increased number of iterations because the learning rate is lower.