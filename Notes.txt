ChatGPT is a language model that predicts sequences of words, characters, or tokens. It understands how words typically relate to one another in the English language.

What is the neural network under the hood that models the sequence of these words? 
GPT stands for Generative Pre-trained Transformer. The Transformer is the neural network architecture that performs the complex computations and processing behind the scenes.

Responses in ChatGPT are generated on a token-by-token basis, where tokens represent smaller subword segments. This means they function at a level that's not purely word-based,
but rather at a chunk level of words. However, in our model, we process it character by character.

We can consider our vocabulary as a set of potential characters that the model can recognize or generate.

When people refer to tokenization, they mean the process of transforming raw text, represented as a string, into a sequence of integers based on a predefined vocabulary of possible elements.

This is just one of many potential encodings or types of tokenizers, and it’s quite basic. There are various other schemas that have been developed in practice. 
For instance, Google employs a tokenizer called SentencePiece, which also converts text into integers but follows a different schema and uses a distinct vocabulary. 
SentencePiece operates at the subword level, meaning it doesn’t encode complete words or individual characters but rather subword units. This subword approach is commonly adopted. 
Additionally, OpenAI has a library called tiktoken that utilizes a byte pair encoding tokenizer, which is the method used by GPT.

Essentially, there is a trade-off between codebook size and sequence length. This means you can opt for very long sequences of integers with a small vocabulary, or you can choose shorter sequences with a larger vocabulary. 
In practice, people commonly use subword encodings to strike a balance between these options.

We want to begin inputting these text sequences or integer sequences into the Transformer for it to learn and recognize patterns. It's important to note that we won’t be feeding the entire text into 
the Transformer all at once, as that would be computationally expensive and impractical. Instead, when training a Transformer with these datasets, we work with smaller portions of the data. 
We randomly sample small chunks from the training set and train on these segments one at a time. Each chunk has a specific length, with a defined maximum length, often referred to as block size in the code, 
though it may also be called context length in other contexts.

We train on all eight examples, utilizing contexts ranging from one to the maximum block size. This approach is not merely for computational efficiency, nor is it just because we already have the sequences available. 
Instead, it is a deliberate strategy to familiarize the Transformer Network with a wide range of contexts. By exposing the model to contexts from as small as one to the full block size—and everything in between—we 
prepare it for more effective inference later on. This means that during the sampling process, we can start with just a single character of context, enabling the Transformer to predict the next character based on 
contexts from one up to the full block size.

We've examined the time dimension of the tensors feeding into the Transformer, but there's one more dimension to consider: the batch dimension. When sampling text chunks, we will feed multiple batches of text stacked 
in a single tensor into the Transformer. This approach is for efficiency, as it keeps the GPUs engaged, leveraging their strength in parallel data processing. Each chunk is processed independently without interaction, 
so let’s generalize this by introducing the batch dimension.

Once we obtain the logits, we concentrate only on the last time step. Instead of maintaining the full shape of B x T x C, we extract the last element along the time dimension, which corresponds to the predictions 
for the next step. This gives us the logits, which we then convert to probabilities using softmax. Finally, we use `torch.multinomial` to sample from these probabilities.

Adam is a more advanced and popular optimizer that works well. A typical good setting for the learning rate is around 3E-4, but for very small networks like this one, much higher learning rates can be effective.

This is the simplest possible model. Currently, the tokens do not interact; they only consider the last character to predict the next one. Now, these tokens need to communicate and incorporate the broader context to improve their predictions. 
This will initiate the Transformer model.

A "bag of words" refers to a method of averaging words, where each of the eight locations holds a word, and we simply average them together. 

We encode not only the identity of tokens but also their positions. Accordingly, there will be a second position embedding table, which is an embedding of block size by an embedding dimension. 
Each position from zero to block size minus one will have its own embedding vector.

Different tokens find other tokens more or less interesting based on data dependency. For instance, a vowel might seek consonants from its past and want information about them to flow in a data-dependent manner. 
Self-attention addresses this by having each token emit two vectors: a query, representing what it seeks, and a key, indicating what it contains. Affinities between tokens are determined through the dot product 
of the keys and queries; when a query aligns closely with a key, the interaction increases, allowing the token to learn more about that specific token compared to others in the sequence.

You can think of X as kind of like private information to this token, so I'm a fifth token at some and I have some identity and my information is kept in Vector X and now for the purposes of the single head here's 
what I'm interested in(Q) here's what I have(K) and if you find me interesting here's what I will communicate to you and that's stored in V and so V is the thing that gets aggregated for the purposes of this
single head.

This is called self-attention because the keys, queries, and values all originate from the same source, X. In this case, the nodes are self-attending. However, attention is generally more versatile. For instance, in encoder-decoder transformers, 
queries can come from X while keys and values may come from a separate external source, such as encoder blocks that provide additional context. Cross attention is utilized when we need to extract information from an external source, while self-attention 
occurs when nodes interact with each other. In this instance, we are using self-attention.

In the context of using softmax in neural networks, it's crucial to ensure that the initial weight values are appropriately set to avoid overly extreme distributions. At initialization, we want the weights to be fairly diffuse; that is, their values should 
be spread out rather than concentrated. This is important because, when softmax is applied to a set of weights that take on very high positive or very low negative values, it can lead to sharp outputs that degenerate into one-hot vectors.
This behavior can be problematic, particularly at the initialization phase. If the values are too extreme and lead to sharp peaks in the softmax output, the network might rely heavily on information from a single node. Ideally, we want each node to aggregate 
information from multiple sources, which allows for a more balanced and effective learning process.
Therefore, controlling the variance at initialization through appropriate scaling of the weights is essential. This scaling helps ensure that the initial distribution does not produce a softmax output that is too peaky, thereby promoting a more effective and 
diverse aggregation of information across the network.

The lower triangular matrix, commonly referred to as 'tril,' is not a parameter of the module in PyTorch. Instead, it is treated as a buffer, which is a type of stateful tensor that does not require gradient updates. To assign this buffer to the module, 
you should use the register_buffer method. This approach ensures that the lower triangular matrix is part of the module's state while not being a learnable parameter.

The self attention can't tolerate very very high learning rates and then I also increased number of iterations because the learning rate is lower.

Multi-head attention applies multiple attention heads in parallel, allowing the model to focus on different aspects of the input simultaneously. The results from these heads are then concatenated and transformed, enabling the model to capture a richer representation of the data.
This concept is somewhat analogous to group convolutions. Rather than performing a single large convolution operation, we break it down into smaller, grouped convolutions. This idea is employed in multi-headed self-attention, where we utilize multiple attention heads instead of a single one. 
So, in this case, we essentially implement several self-attention mechanisms in parallel, which allows for capturing different aspects of the input data simultaneously.

Self-attention serves as a communication mechanism that aggregates contextual information across the input data. After this process gathers insights, the feed-forward network processes each element individually, applying transformations to refine and enhance the representations. 
In this way, self-attention focuses on relationships within the data, while the feed-forward network enables the model to interpret and act on that information more effectively.

The computation flows from top to bottom, utilizing a residual pathway. Within this structure, you can diverge from the residual pathway to perform additional computations and then rejoin the residual pathway through addition. This means that the transformation from inputs to targets relies primarily on addition operations.

This approach is advantageous because, during backpropagation the addition operation distributes gradients equally across both branches that feed into it. Consequently, the supervision signals (or gradients from the loss) traverse each addition node, reaching the input while also branching out into the residual blocks. 
Essentially, there exists a gradient "superhighway" that allows direct access from supervision to the input, ensuring an efficient flow of gradients without obstruction.

Furthermore, the residual blocks are typically initialized to contribute very little, if anything, to the residual pathway at the outset. They are designed this way so that, initially, they appear almost non-intrusive. However, as optimization progresses, 
these blocks gradually become active and start to contribute meaningfully. This initialization strategy allows for an unimpeded flow of gradients from supervision to the inputs, which dramatically facilitates the optimization process.

In Layer Normalisation we normalise each row unlike BatchNorm that normalises each column.

In the original paper, the ADD and normalization were applied after the transformation. However, it has become more common to apply layer normalization before the transformation. This results in a rearrangement of the layer normalization process, 
leading to what's known as the "pre-norm" formulation.

Dropout is a regularization technique introduced in the 2014 paper "Dropout: A Simple Way to Prevent Neural Networks from Overfitting" by Nitish Srivastava and co-authors. The method involves randomly deactivating a subset of neurons during each training iteration, effectively setting their outputs to zero.

This random deactivation occurs during both the forward and backward passes, which means that each training step uses a different configuration of active neurons. As a result, dropout trains an ensemble of diverse sub-networks, each learning to make predictions based on various combinations of neurons.

During testing, all neurons are fully active, allowing the model to utilize the full network’s capacity. You can think of this at test time as merging the knowledge learned from all the sub-networks into a single model.

It's essential to recognize dropout primarily as an effective regularization technique.